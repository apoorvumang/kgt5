CUDA_VISIBLE_DEVICES=1 python3 eval_accelerate.py --prefix fbwq_qa_half_kg_lm_pt \
--dataset fbwq_half --batch_size 200 --beam_size 1 --num_predictions 1 \
--max_points -1 --checkpoint 5000 --eval_split test \
--task qa --hops 1 \
--tokenizer t5 --save_file scores_temp


CUDA_VISIBLE_DEVICES=1 python3 eval_accelerate.py --prefix fbwq_half_kg_try2  \
--dataset fbwq_half --batch_size 2 --beam_size 1 --num_predictions 100 \
--max_points 500 --checkpoint 220000 --eval_split test \
--tokenizer fbwq_with_pad --save_file scores_temp2




OMP_NUM_THREADS=1 CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python3 -m torch.distributed.launch --nproc_per_node 7 --use_env ./main_accelerate.py \
--save_prefix wd5m_test \
--model_size small --dataset wikidata5m \
--tokenizer sentencepiece \
--batch_size 100 --save_steps 2500 \
--loss_steps 250 \
--epochs 5



OMP_NUM_THREADS=2 CUDA_VISIBLE_DEVICES=1,4 python3 -m torch.distributed.launch --nproc_per_node 2 --use_env ./main_accelerate.py \
--save_prefix fbwq_half_lego_kg \
--model_size small --dataset fbwq_half_lego \
--tokenizer fbwq_half_lego_with_pad \
--batch_size 100 --save_steps 5000 \
--loss_steps 250 \
--epochs 50


CUDA_VISIBLE_DEVICES=5 python3 main_accelerate.py \
--save_prefix fbwq_half_lego_qa_base_try3 \
--model_size base --dataset fbwq_half_lego \
--tokenizer t5 \
--batch_size 40 --save_steps 1000 \
--loss_steps 250 \
--task qa --hops 1 \
--epochs 50 --load_checkpoint fbwq_half_lego_kg_base/230000.pt



CUDA_VISIBLE_DEVICES=2 python3 eval_accelerate.py --prefix fbwq_half_lego_qa_base_try3 \
--dataset fbwq_half_lego --batch_size 100 --beam_size 2 --num_predictions 1 \
--max_points -1 --checkpoint 8000 --eval_split test \
--task qa --hops 1 \
--tokenizer t5 --save_file scores_temp



CUDA_VISIBLE_DEVICES=4 python3 main_accelerate.py \
--save_prefix fbwq_half_lego_qa_base_try4 \
--model_size base --dataset fbwq_half_lego \
--tokenizer t5 \
--batch_size 40 --save_steps 1000 \
--loss_steps 250 \
--task qa --hops 1 \
--epochs 50 --load_checkpoint fbwq_half_lego_kg_base_try2/45000.pt



OMP_NUM_THREADS=2 CUDA_VISIBLE_DEVICES=1,2,3,4 python3 -m torch.distributed.launch --nproc_per_node 4 --use_env ./main_accelerate.py \
--save_prefix fbwq_half_lego_qa_base_mixed \
--model_size base --dataset fbwq_half_lego_mixed \
--tokenizer t5 \
--batch_size 40 --save_steps 10000 \
--loss_steps 250 \
--epochs 50 \
--load_checkpoint fbwq_half_lego_kg_base/235000.pt




CUDA_VISIBLE_DEVICES=1 python3 eval_accelerate.py --prefix codex-s_small_1gpu \
--dataset codex-s --batch_size 2 --beam_size 1 --num_predictions 200 \
--max_points -1 --checkpoint 81000 --eval_split test \
--tokenizer t5 --save_file scores_codex-s


fbwq_half_lego_qa_base_mixed



CUDA_VISIBLE_DEVICES=1 python3 eval_accelerate.py --prefix fbwq_half_lego_qa_base_mixed \
--dataset fbwq_half_lego --batch_size 20 --beam_size 4 --num_predictions 1 \
--max_points -1 --checkpoint 70000 --eval_split test \
--task qa --hops
--tokenizer t5 --save_file scores_fbwq_half_lego_1predict



CUDA_VISIBLE_DEVICES=3 python3 eval_accelerate.py --prefix wd5m-sentencepiece-rp  \
--dataset fbwq_half --batch_size 1 --beam_size 1 --num_predictions 200 \
--max_points 100 --checkpoint 530000 --eval_split test \
--tokenizer sentencepiece --save_file scores_wd5m_beam


CUDA_VISIBLE_DEVICES=2 python3 eval_accelerate.py --prefix codex-s_small_1gpu \
--dataset codex-s --batch_size 50 --beam_size 5 --num_predictions 5 --max_points -1 \
--checkpoint 51000 --eval_split test --tokenizer t5 \
--save_file scores_codex-s_s




OMP_NUM_THREADS=2 CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m torch.distributed.launch --nproc_per_node 4 --use_env ./main_accelerate.py \
--save_prefix wd5m_v2_small \
--model_size small --dataset wikidata5m_v2 \
--tokenizer sp_wd5m_v2 \
--batch_size 80 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 



CUDA_VISIBLE_DEVICES=3 python3 eval_accelerate.py --prefix wd5m_v2_small  \
--dataset wikidata5m_v2 --batch_size 10 --beam_size 1 --num_predictions 20 \
--max_points 100 --checkpoint 500000 --eval_split test \
--tokenizer sp_wd5m_v2 --save_file scores_wd5m_v2



# for training sp_wd5m_v3 we used 4M sentences and following

#### begin

sp_data_fname = '/scratche/home/apoorv/transformer-kgc/data/wikidata5m_v3/lines_for_sp_training.txt'
vocab_size = 32000
user_defined_symbols = ['|TAIL|', '|HEAD|', '|||']
# user_defined_symbols = ['|||']
sp = spm.SentencePieceTrainer.train(input=sp_data_fname,
                                    model_prefix='sp_wd5m_v3', 
                                    vocab_size=vocab_size, 
                                    user_defined_symbols=user_defined_symbols,
                                    model_type='BPE',
                                    character_coverage=1.0,
                                    split_by_whitespace=False, # better compress relations?
                                    add_dummy_prefix=True,
                                    byte_fallback=True,
                                    pad_id=3,
                                    train_extremely_large_corpus=True # does nothing for BPE afaik
                                   )

#### end


OMP_NUM_THREADS=2 CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m torch.distributed.launch --nproc_per_node 4 --use_env ./main_accelerate.py \
--save_prefix wd5m_v3_base \
--model_size base --dataset wikidata5m_v3 \
--tokenizer sp_wd5m_v3 \
--batch_size 80 --save_steps 40000 \
--loss_steps 250 \
--epochs 50 --pad_to_max


accelerate launch main_accelerate.py --save_prefix wd5m_v3_base \
--model_size base --dataset wikidata5m_v3 \
--tokenizer sp_wd5m_v3 \
--batch_size 80 --save_steps 40000 \
--loss_steps 250 \
--epochs 50 --pad_to_max



accelerate-launch main_accelerate.py --save_prefix codexm \
--model_size small --dataset codex-m \
--tokenizer sp_wd5m_v3 \
--batch_size 200 --save_steps 40000 \
--loss_steps 250 \
--epochs 50 --pad_to_max \
--num_workers 24




OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m torch.distributed.launch --nproc_per_node 4 --use_env ./main_accelerate.py \
--save_prefix wd5m_v2_small \
--model_size small --dataset wikidata5m_v2 \
--tokenizer sp_wd5m_v2 \
--batch_size 80 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --load_checkpoint wd5m_v2_small/820000.pt \
--start_steps 820000 --pad_to_max


 OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node 2 --use_env ./main_accelerate.py \
--save_prefix test_ld \
--model_size small --dataset codex-m \
--tokenizer sp_wd5m_v2 \
--batch_size 200 --save_steps 20000 \
--loss_steps 250 \
--epochs 50


CUDA_VISIBLE_DEVICES=0 python3 ./main_accelerate.py \
--save_prefix test_ld \
--model_size small --dataset codex-m \
--tokenizer t5 \
--batch_size 200 --save_steps 20000 \
--loss_steps 250 \
--epochs 50



 python3 ./main_accelerate.py \
--save_prefix test_ld \
--model_size small --dataset codex-m \
--tokenizer t5 \
--batch_size 200 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --pad_to_max


OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m torch.distributed.launch --nproc_per_node 4 --use_env ./main_accelerate.py \
--save_prefix wd5m_v2_small \
--model_size small --dataset wikidata5m_v2 \
--tokenizer sp_wd5m_v2 \
--batch_size 220 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --load_checkpoint wd5m_v2_small/820000.pt \
--start_steps 820000



OMP_NUM_THREADS=2 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node 2 --use_env ./main_accelerate.py \
--save_prefix wd5m_v3_small \
--model_size small --dataset wikidata5m_v3 \
--tokenizer sp_wd5m_v3 \
--batch_size 200 --save_steps 20000 \
--loss_steps 250 \
--epochs 50


 OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1,2,3 python3 -m torch.distributed.launch --nproc_per_node 4 --use_env ./main_accelerate.py \
--save_prefix wd5m_v2_small \
--model_size small --dataset wikidata5m_v2 \
--tokenizer sp_wd5m_v2 \
--batch_size 220 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --load_checkpoint wd5m_v2_small/820000.pt \
--start_steps 820000 --pad_to_max




 OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node 2 --use_env ./main_accelerate.py \
--save_prefix wd5m_v2_small \
--model_size small --dataset wikidata5m_v2 \
--tokenizer sp_wd5m_v2 \
--batch_size 200 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --load_checkpoint wd5m_v2_small/820000.pt \
--start_steps 820000





python3 ./main_accelerate.py \
--save_prefix wd5m_v3_base \
--model_size base --dataset wikidata5m_v3 \
--tokenizer sp_wd5m_v3 \
--batch_size 100 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --pad_to_max




python3 ./xla_spawn.py \
  --num_cores 8 \
    examples/pytorch/translation/run_translation.py \
    --model_name_or_path t5-base \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix "translate English to Romanian: " \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=16 \
    --per_device_eval_batch_size=16 \
    --overwrite_output_dir \
    --predict_with_generate \
    --tpu_metrics_debug \
    --logging_dir ./tensorboard-metrics \
    --pad_to_max_length \
    --save_steps 500000 \
    --max_source_length=128 

python3 ./xla_spawn.py \
  --num_cores 8 \
    examples/pytorch/translation/run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix "translate English to Romanian: " \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=256 \
    --per_device_eval_batch_size=256 \
    --overwrite_output_dir \
    --predict_with_generate \
    --pad_to_max_length \
    --max_source_length=32 \
    --max_target_length=24 \  
    --save_steps 50000




python examples/xla_spawn.py \
  --num_cores 8 \
  examples/language-modeling/run_mlm.py \
  --dataset_name wikitext \
  --dataset_config_name wikitext-103-raw-v1 \
  --max_seq_length 512 \
  --pad_to_max_length \
  --logging_dir ./tensorboard-metrics \
  --cache_dir ./cache_dir \
  --do_train \
  --do_eval \
  --overwrite_output_dir \
  --output_dir language-modeling \
  --overwrite_cache \
  --tpu_metrics_debug \
  --model_name_or_path bert-large-uncased \
  --num_train_epochs 3 \
  --per_device_train_batch_size 8 \
  --per_device_eval_batch_size 8 \
  --save_steps 500000



python3 ./xla_spawn.py \
  --num_cores 8 \
  examples/pytorch/language-modeling/run_mlm.py \
  --dataset_name wikitext \
  --dataset_config_name wikitext-103-raw-v1 \
  --max_seq_length 512 \
  --cache_dir ./cache_dir \
  --do_train \
  --do_eval \
  --overwrite_output_dir \
  --output_dir language-modeling \
  --overwrite_cache \
  --model_name_or_path bert-large-uncased \
  --num_train_epochs 3 \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 4 \



python3  /home/apoorv/.local/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py launch examples/pytorch/translation/run_translation_no_trainer.py  --model_name_or_path t5-base     --do_train        --source_lang en     --target_lang ro     --source_prefix "translate English to Romanian: "     --dataset_name wmt16     --dataset_config_name ro-en     --output_dir /tmp/tst-translation     --per_device_train_batch_size=256     --per_device_eval_batch_size=256     --overwrite_output_dir     --predict_with_generate     --pad_to_max_length     --max_source_length=32     --max_target_length=24


python3  /home/apoorv/.local/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py launch examples/pytorch/translation/run_translation_no_trainer.py \
    --model_name_or_path t5-base \
    --source_lang en \
    --target_lang ro \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir ~/tmp/tst-translation \
    --per_device_train_batch_size 256 \
    --max_source_length 32 \
    --max_target_length 32 \
    --pad_to_max_length=True \
    --max_length 32



python3 ./xla_spawn.py \
  --num_cores 8 \
    examples/pytorch/translation/run_translation.py \
    --model_name_or_path t5-base \
    --do_train \
    --train_file data/wikidata5m_v3/json_format/train.json \
    --output_dir /tmp/train-full-seq2seq-t5-base \
    --per_device_train_batch_size=128 \
    --overwrite_output_dir \
    --pad_to_max_length \
    --max_source_length=32 \
    --max_target_length=32 \
    --save_steps 10000 \
    --cache_dir ./cache_dir \
    --preprocessing_num_workers 16



    python3 examples/pytorch/translation/run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --train_file data/wikidata5m_v3/json_format/train.json \
    --output_dir /tmp/train-full-seq2seq-t5-small \
    --per_device_train_batch_size=128 \
    --overwrite_output_dir \
    --max_source_length=32 \
    --max_target_length=32 \
    --save_steps 20000 \
    --cache_dir ./cache_dir \
    --preprocessing_num_workers 16


python3 ./xla_spawn.py \
  --num_cores 8 \
    examples/pytorch/translation/run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --train_file data/wikidata5m_v3/json_format/train.json \
    --output_dir /tmp/train-full-seq2seq-t5-small \
    --per_device_train_batch_size=512 \
    --overwrite_output_dir \
    --pad_to_max_length \
    --max_source_length=32 \
    --max_target_length=32 \
    --save_steps 10000 \
    --cache_dir ./cache_dir \
    --preprocessing_num_workers 16



CUDA_VISIBLE_DEVICES=1,2,3 python3 -m torch.distributed.launch --nproc_per_node 3 --use_env ./eval_accelerate_parallel.py \
--prefix wd5m_v3_small  \
--dataset wikidata5m_v3 --batch_size 2 --beam_size 1 --num_predictions 250 \
--max_points -1 --checkpoint 500000 --eval_split test \
--tokenizer sp_wd5m_v3 --save_file scores_wd5m_v3



 OMP_NUM_THREADS=4 CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node 2 --use_env ./main_accelerate.py \
--save_prefix wnrr_small \
--model_size small --dataset wnrr \
--tokenizer t5 \
--batch_size 80 --save_steps 10000 \
--loss_steps 250 \
--epochs 50


CUDA_VISIBLE_DEVICES=2 python3 eval_accelerate.py --prefix wnrr_small \
--dataset wnrr --batch_size 1 --beam_size 1 --num_predictions 400 \
--max_points -1 --checkpoint 10000 --eval_split test \
--tokenizer t5 --save_file scores_wnrr_small_400


CUDA_VISIBLE_DEVICES=1 python3 ./main_accelerate.py \
--save_prefix cwq_full_kgc_small \
--model_size small --dataset cwq_full \
--tokenizer t5 \
--batch_size 160 --save_steps 20000 \
--loss_steps 250 \
--epochs 50 --max_input_sequence_length 60 --max_output_sequence_length 20
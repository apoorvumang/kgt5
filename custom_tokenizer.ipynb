{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15caa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# vocab_size = 2000\n",
    "# tokenizer = Tokenizer(BPE(unk_token=\"<unk>\", single_world=True))\n",
    "# trainer = BpeTrainer(special_tokens=['<s>',\n",
    "#  '</s>',\n",
    "#  '<pad>',\n",
    "#  '<unk>',\n",
    "#  '<special0>',\n",
    "#  '<special1>',\n",
    "#  '<special2>',\n",
    "#  '<special3>',\n",
    "#  '<special4>'], vocab_size=vocab_size)\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# files = [f\"data/codex-m/{split}.txt\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "# tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29acd760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "vocab_size = 30000\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"<unk>\", single_world=True))\n",
    "trainer = WordPieceTrainer(special_tokens=['<s>',\n",
    " '</s>',\n",
    " '<pad>',\n",
    " '<unk>',\n",
    " '<special0>',\n",
    " '<special1>',\n",
    " '<special2>',\n",
    " '<special3>',\n",
    " '<special4>'], vocab_size=vocab_size)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# changing to train_with_rp.txt for relation prediction\n",
    "# files = [f\"data/codex-m/{split}.txt\" for split in [\"test\", \"train_with_rp\", \"valid\"]]\n",
    "files = [f\"data/wikidata5m/{split}.txt\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02796e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import XLMTokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def saveBPETokenizer(tokenizer, prefix):\n",
    "    # make directory for tokenizer\n",
    "    path = os.path.join('data/bpe', prefix)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    # save + load data\n",
    "    main_fname = 'data/bpe/{}/main.json'.format(prefix)\n",
    "    tokenizer.save(main_fname)\n",
    "    f = open(main_fname, 'r')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    #save data in format required by transformers.PretrainedTokenizer\n",
    "    vocab_fname = 'data/bpe/{}/vocab.json'.format(prefix)\n",
    "    merges_fname = 'data/bpe/{}/merges.txt'.format(prefix)\n",
    "    json_object = json.dumps(data['model']['vocab'])\n",
    "    with open(vocab_fname, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    f = open(merges_fname, 'w')\n",
    "    for x in data['model']['merges']:\n",
    "        f.write(x +'\\n')\n",
    "    f.close()\n",
    "    print('Saved')\n",
    "    \n",
    "def saveWordPieceTokenizer(tokenizer, prefix):\n",
    "    # make directory for tokenizer\n",
    "    path = os.path.join('data/wordpiece', prefix)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    # save + load data\n",
    "    main_fname = 'data/wordpiece/{}/main.json'.format(prefix)\n",
    "    tokenizer.save(main_fname)\n",
    "    print('Saved')\n",
    "\n",
    "\n",
    "\n",
    "def loadBPETokenizer(prefix):\n",
    "    vocab_fname = 'data/bpe/{}/vocab.json'.format(prefix)\n",
    "    merges_fname = 'data/bpe/{}/merges.txt'.format(prefix)\n",
    "#     tokenizer = XLMTokenizer(vocab_file=vocab_fname, merges_file=merges_fname)\n",
    "    tokenizer = GPT2Tokenizer(vocab_file=vocab_fname, merges_file=merges_fname, \n",
    "                              unk_token='<unk>', \n",
    "                              bos_token='<s>', \n",
    "                              eos_token='</s>', \n",
    "                              add_prefix_space=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bbea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# saveBPETokenizer(tokenizer, 'codexm{}'.format(vocab_size))\n",
    "saveWordPieceTokenizer(tokenizer, 'wikidata5m_{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96f6350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = train_data[0]\n",
    "text = 'hello world'\n",
    "tokenizer.encode_batch([\"predict tail: obama | united states of america |\", \"How are you üòÅ ?\", text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fa426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '</s>', '<pad>', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello world  </s>'\n",
    "tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\")\n",
    "x = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\", text])\n",
    "x[2].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f625229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode('predict relation: obama | united states of america | </s>').ids\n",
    "y = tokenizer.encode('fsadlk jfasdkf jsadlkfjas lkdjf').ids\n",
    "out = tokenizer.decode_batch([x,y], skip_special_tokens=True)\n",
    "# for x in out:\n",
    "#     x = x.replace(' ##', '')\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b876eac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predict',\n",
       " 'tail',\n",
       " ':',\n",
       " 'obama',\n",
       " '|',\n",
       " 'united',\n",
       " 'states',\n",
       " 'of',\n",
       " 'america',\n",
       " '|',\n",
       " '</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tokenizer.encode('predict tail: obama | united states of america | </s>')\n",
    "y.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b9cb2936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<pad>').ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "09c0364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = loadBPETokenizer('codexm{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "89cd85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c800ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tk(text, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "# tokenized_ids = [0 if token == None else token for token in tokenized.input_ids]\n",
    "# ''.join(tk.convert_ids_to_tokens(tokenized_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17f11abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "bee3b882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [1,2,3,4]\n",
    "x = np.array(x)\n",
    "np.stack([x,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b029923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 65,\n",
       " 42,\n",
       " 36,\n",
       " 53,\n",
       " 3,\n",
       " 77,\n",
       " 31,\n",
       " 3,\n",
       " 34,\n",
       " 3,\n",
       " 1115,\n",
       " 56,\n",
       " 3,\n",
       " 1587,\n",
       " 3,\n",
       " 617,\n",
       " 3,\n",
       " 60,\n",
       " 3,\n",
       " 193,\n",
       " 3,\n",
       " 111,\n",
       " 3,\n",
       " 60,\n",
       " 3,\n",
       " 554,\n",
       " 3,\n",
       " 777,\n",
       " 107,\n",
       " 122,\n",
       " 211]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BatchEncoding\n",
    "da = {}\n",
    "da['input_ids'] = x.input_ids\n",
    "da['attention_mask'] = x.attention_mask\n",
    "da2 = BatchEncoding(da)\n",
    "da2.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9cf1d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "fname = 'data/codex-m/train.txt'\n",
    "f = open(fname, 'r')\n",
    "for line in f:\n",
    "    train_data.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "11dcd2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.unk_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9720b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "fname = \"data/tokenizer-codexm-2k.json\"\n",
    "f = open(fname, 'r')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf3f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'pretrained_vocab_files_map ': data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99a56b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling PreTrainedTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-79b53b903370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                     )\n\u001b[1;32m   1702\u001b[0m                 )\n\u001b[0;32m-> 1703\u001b[0;31m                 \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m                 \u001b[0mvocab_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# tokenizer =  XLMTokenizer(vocab_file='vocab.json', merges_file='merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "390ffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmtk = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d939ade4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<unk>',\n",
       " '<special0>',\n",
       " '<special1>',\n",
       " '<special2>',\n",
       " '<special3>',\n",
       " '<special4>']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmtk.convert_ids_to_tokens([0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c88b6b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLMTokenizer' object has no attribute 'special_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-0249e582fd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxlmtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLMTokenizer' object has no attribute 'special_tokens'"
     ]
    }
   ],
   "source": [
    "xlmtk.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53424246",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytk = XLMTokenizer(vocab_file='vocab.json', merges_file='merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd5af210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<s>', '</s>', '<pad>', '<mask>', '!']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytk.convert_ids_to_tokens([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27a317f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'codexm2k' to PyBool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8b6366dbbf7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/bpe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'codexm2k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'codexm2k' to PyBool"
     ]
    }
   ],
   "source": [
    "tokenizer.save('data/bpe', 'codexm2k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffc1c2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tokenizer.save(self, pretty=False)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d478aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b5e6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('merges.txt', 'w')\n",
    "for x in data['model']['merges']:\n",
    "    f.write(x +'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62578e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64f2cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk2 = XLMTokenizer(vocab_file='vocab.json', merges_file='merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86ad5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c02f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8addc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5tk = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38410744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk.convert_tokens_to_ids('<e>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7885e361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [21820, 3, 2, 7, 3155, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk('hello <s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "341279c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '</s>', '<unk>', '‚ñÅ', 'X', '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk.convert_ids_to_tokens([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a29a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

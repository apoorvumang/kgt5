{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ea18291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "sp2 = spm.SentencePieceTrainer.train(input='data/wikidata5m/full.txt', \n",
    "                               model_prefix='wd5m_with_pad', vocab_size=vocab_size, \n",
    "                               user_defined_symbols=['<pad>'],\n",
    "                               train_extremely_large_corpus=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a2c5125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = spm.SentencePieceProcessor(model_file='wd5m_with_pad.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6241d210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp2['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c63459b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 5, 27351, 21172, 4, 89, 258, 4]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp = spm.SentencePieceProcessor(model_file='wd5m.model')\n",
    "sp = spm.SentencePieceProcessor(model_file='wd5m_with_pad.model')\n",
    "sp.encode('predict head: barack obama | position held |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "32e65d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predict head: barack obama | position held |<pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([[6, 8, 5, 27351, 21172, 4, 89, 258, 4, 3,3,3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c3fe5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d8d539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02a394db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predict'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de366a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    5,     7,     4, 21171,     3,    88,   257,     3,     2],\n",
       "       [    5,     8,     4,    68,     3,    68,     3,     2,     0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = 'predict head: obama | position held |'\n",
    "input2 = 'predict tail: f | f |'\n",
    "out = sp.encode([input1, input2])\n",
    "for x in out:\n",
    "    x.append(sp['</s>'])\n",
    "length = max(map(len, out))\n",
    "y = np.array([xi+[sp['<pad>']]*(length-len(xi)) for xi in out])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05ace82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(x) for x in out]\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec9975b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(lengths)\n",
    "attention_mask = np.zeros((len(lengths), max_len), dtype=np.long)\n",
    "for i, l in enumerate(lengths):\n",
    "    attention_mask[i][:l] = np.ones((l), dtype=np.long)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a8bbc820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,     7,     4, 21171,     3,    88,   257,     3,     2],\n",
       "        [    5,     8,     4,    68,     3,    68,     3,     2,     0]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.LongTensor(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e15ea52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ‚Åá  ‚Åá  ‚Åá '"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "916771d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lengths = [len(x) for x in out]\n",
    "max_len = max(lengths)\n",
    "# np.pad(out, (1,1))\n",
    "x = np.zeros((len(out), max_len), dtype=np.long)\n",
    "x[0] = out[0]\n",
    "x[1][:lengths[1]] = out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f1dae72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8318, 68, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello f'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sp.encode('hello f')\n",
    "x.append(sp['</s>'])\n",
    "x = np.array(x)\n",
    "x = x.tolist()\n",
    "print(x)\n",
    "sp.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "006d18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅpredict', '‚ñÅhead', ':', '‚ñÅobama', '‚ñÅ|', '‚ñÅposition', '‚ñÅheld', '‚ñÅ|']\n",
      "['‚ñÅpredict', '‚ñÅhead', ':', '‚ñÅobama', '‚ñÅ|', '‚ñÅposition', '‚ñÅheld', '‚ñÅ|']\n",
      "['‚ñÅpredict', '‚ñÅhead', ':', '‚ñÅobama', '‚ñÅ|', '‚ñÅposition', '‚ñÅhel', 'd', '‚ñÅ|']\n",
      "['‚ñÅpredict', '‚ñÅhead', ':', '‚ñÅobama', '‚ñÅ|', '‚ñÅposition', '‚ñÅheld', '‚ñÅ|']\n",
      "['‚ñÅpredict', '‚ñÅhead', ':', '‚ñÅob', 'ama', '‚ñÅ|', '‚ñÅposition', '‚ñÅheld', '‚ñÅ|']\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    x = sp.encode('predict head: obama | position held |', out_type=str, enable_sampling=True, alpha=0.5, nbest_size=-1)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02796e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import XLMTokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def saveBPETokenizer(tokenizer, prefix):\n",
    "    # make directory for tokenizer\n",
    "    path = os.path.join('data/bpe', prefix)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    # save + load data\n",
    "    main_fname = 'data/bpe/{}/main.json'.format(prefix)\n",
    "    tokenizer.save(main_fname)\n",
    "    f = open(main_fname, 'r')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    #save data in format required by transformers.PretrainedTokenizer\n",
    "    vocab_fname = 'data/bpe/{}/vocab.json'.format(prefix)\n",
    "    merges_fname = 'data/bpe/{}/merges.txt'.format(prefix)\n",
    "    json_object = json.dumps(data['model']['vocab'])\n",
    "    with open(vocab_fname, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    f = open(merges_fname, 'w')\n",
    "    for x in data['model']['merges']:\n",
    "        f.write(x +'\\n')\n",
    "    f.close()\n",
    "    print('Saved')\n",
    "    \n",
    "def saveWordPieceTokenizer(tokenizer, prefix):\n",
    "    # make directory for tokenizer\n",
    "    path = os.path.join('data/wordpiece', prefix)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    # save + load data\n",
    "    main_fname = 'data/wordpiece/{}/main.json'.format(prefix)\n",
    "    tokenizer.save(main_fname)\n",
    "    print('Saved')\n",
    "\n",
    "\n",
    "\n",
    "def loadBPETokenizer(prefix):\n",
    "    vocab_fname = 'data/bpe/{}/vocab.json'.format(prefix)\n",
    "    merges_fname = 'data/bpe/{}/merges.txt'.format(prefix)\n",
    "#     tokenizer = XLMTokenizer(vocab_file=vocab_fname, merges_file=merges_fname)\n",
    "    tokenizer = GPT2Tokenizer(vocab_file=vocab_fname, merges_file=merges_fname, \n",
    "                              unk_token='<unk>', \n",
    "                              bos_token='<s>', \n",
    "                              eos_token='</s>', \n",
    "                              add_prefix_space=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bbea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# saveBPETokenizer(tokenizer, 'codexm{}'.format(vocab_size))\n",
    "saveWordPieceTokenizer(tokenizer, 'wikidata5m_{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96f6350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = train_data[0]\n",
    "text = 'hello world'\n",
    "tokenizer.encode_batch([\"predict tail: obama | united states of america |\", \"How are you üòÅ ?\", text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fa426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '</s>', '<pad>', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello world  </s>'\n",
    "tokenizer.enable_padding(pad_id=2, pad_token=\"<pad>\")\n",
    "x = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\", text])\n",
    "x[2].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f625229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.encode('predict relation: obama | united states of america | </s>').ids\n",
    "y = tokenizer.encode('fsadlk jfasdkf jsadlkfjas lkdjf').ids\n",
    "out = tokenizer.decode_batch([x,y], skip_special_tokens=True)\n",
    "# for x in out:\n",
    "#     x = x.replace(' ##', '')\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b876eac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predict',\n",
       " 'tail',\n",
       " ':',\n",
       " 'obama',\n",
       " '|',\n",
       " 'united',\n",
       " 'states',\n",
       " 'of',\n",
       " 'america',\n",
       " '|',\n",
       " '</s>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tokenizer.encode('predict tail: obama | united states of america | </s>')\n",
    "y.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b9cb2936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<pad>').ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "09c0364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = loadBPETokenizer('codexm{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "89cd85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c800ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tk(text, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "# tokenized_ids = [0 if token == None else token for token in tokenized.input_ids]\n",
    "# ''.join(tk.convert_ids_to_tokens(tokenized_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17f11abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "bee3b882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [1,2,3,4]\n",
    "x = np.array(x)\n",
    "np.stack([x,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b029923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 65,\n",
       " 42,\n",
       " 36,\n",
       " 53,\n",
       " 3,\n",
       " 77,\n",
       " 31,\n",
       " 3,\n",
       " 34,\n",
       " 3,\n",
       " 1115,\n",
       " 56,\n",
       " 3,\n",
       " 1587,\n",
       " 3,\n",
       " 617,\n",
       " 3,\n",
       " 60,\n",
       " 3,\n",
       " 193,\n",
       " 3,\n",
       " 111,\n",
       " 3,\n",
       " 60,\n",
       " 3,\n",
       " 554,\n",
       " 3,\n",
       " 777,\n",
       " 107,\n",
       " 122,\n",
       " 211]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BatchEncoding\n",
    "da = {}\n",
    "da['input_ids'] = x.input_ids\n",
    "da['attention_mask'] = x.attention_mask\n",
    "da2 = BatchEncoding(da)\n",
    "da2.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9cf1d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "fname = 'data/codex-m/train.txt'\n",
    "f = open(fname, 'r')\n",
    "for line in f:\n",
    "    train_data.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "11dcd2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.unk_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9720b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "fname = \"data/tokenizer-codexm-2k.json\"\n",
    "f = open(fname, 'r')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf3f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'pretrained_vocab_files_map ': data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99a56b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling PreTrainedTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-79b53b903370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                     )\n\u001b[1;32m   1702\u001b[0m                 )\n\u001b[0;32m-> 1703\u001b[0;31m                 \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m                 \u001b[0mvocab_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# tokenizer =  XLMTokenizer(vocab_file='vocab.json', merges_file='merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "390ffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmtk = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d939ade4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<unk>',\n",
       " '<special0>',\n",
       " '<special1>',\n",
       " '<special2>',\n",
       " '<special3>',\n",
       " '<special4>']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmtk.convert_ids_to_tokens([0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c88b6b29",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XLMTokenizer' object has no attribute 'special_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-0249e582fd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxlmtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLMTokenizer' object has no attribute 'special_tokens'"
     ]
    }
   ],
   "source": [
    "xlmtk.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "53424246",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytk = XLMTokenizer(vocab_file='vocab.json', merges_file='merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd5af210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<s>', '</s>', '<pad>', '<mask>', '!']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytk.convert_ids_to_tokens([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27a317f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'codexm2k' to PyBool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8b6366dbbf7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/bpe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'codexm2k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'codexm2k' to PyBool"
     ]
    }
   ],
   "source": [
    "tokenizer.save('data/bpe', 'codexm2k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffc1c2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tokenizer.save(self, pretty=False)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d478aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b5e6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('merges.txt', 'w')\n",
    "for x in data['model']['merges']:\n",
    "    f.write(x +'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62578e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64f2cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk2 = XLMTokenizer(vocab_file='vocab.json', merges_file='merges.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86ad5783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c02f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8addc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5tk = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38410744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk.convert_tokens_to_ids('<e>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7885e361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [21820, 3, 2, 7, 3155, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk('hello <s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "341279c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '</s>', '<unk>', '‚ñÅ', 'X', '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5tk.convert_ids_to_tokens([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a29a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

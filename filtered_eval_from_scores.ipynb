{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aba755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d5f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def numLines(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "def loadData(filename, max_points):\n",
    "    file_len = numLines(filename)\n",
    "    f = open(filename, 'r')\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in tqdm(range(file_len)):\n",
    "        if i == max_points:\n",
    "            break\n",
    "        line = f.readline()\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        inputs.append(line[0])\n",
    "        outputs.append(line[1])\n",
    "    data = {'inputs': inputs, 'outputs': outputs}\n",
    "    return data\n",
    "        \n",
    "def load_entity_strings(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def get_entity_wd_id_dict(filename):\n",
    "    out = {}\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        out[line[1]] = line[0]\n",
    "    return out\n",
    "    \n",
    "\n",
    "def create_filter_dict(data) -> Dict[str, int]:\n",
    "    filter_dict = defaultdict(list)\n",
    "    for input, output in zip(data[\"inputs\"], data[\"outputs\"]):\n",
    "        filter_dict[input].append(output)\n",
    "    return filter_dict\n",
    "\n",
    "def getAllFilteringEntities(input, filter_dicts):\n",
    "    entities = []\n",
    "    splits = ['train', 'test', 'valid']\n",
    "    for s in splits:\n",
    "        entities.extend(filter_dicts[s][input])\n",
    "    return list(set(entities))\n",
    "\n",
    "def wikidata_link_from_id(id):\n",
    "    uri = 'https://www.wikidata.org/wiki/' + id\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wikidata5m'\n",
    "entity_strings = load_entity_strings(os.path.join(\"data\", dataset_name, \"entity_strings.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ee97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_strings_set = set(entity_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51647083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5676a56548743cdb9499b2c782e50c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42687362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284f01fba3f44400a9d3d1a69641a493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959940afcd364e028a9fcaaf4e4b61b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {}\n",
    "splits = ['train', 'valid', 'test']\n",
    "dataset_name = 'wikidata5m'\n",
    "for split in splits:\n",
    "    data[split] = loadData(os.path.join('data', dataset_name, split + '.txt'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ed9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2wdid = get_entity_wd_id_dict('data/wikidata5m/aliases.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a7b2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4771027'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2wdid['antarctica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d08940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the guns of navarone\n",
      "guns of navarone\n",
      "the guns of navarone (film)\n"
     ]
    }
   ],
   "source": [
    "y = 'guns of navarone'\n",
    "for k in e2wdid.keys():\n",
    "    if y in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616e3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dicts = {}\n",
    "splits = ['train', 'valid', 'test']\n",
    "for split in splits:\n",
    "    filter_dicts[split] = create_filter_dict(data[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a5a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'scores.pickle'\n",
    "# fname = 'scores/scores_full_base2.pickle'\n",
    "fname = 'scores/scores_wd5m_500.pickle'\n",
    "# fname = 'scores_500_base_trie.pickle'\n",
    "scores_data = pickle.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bff88b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0195e5ed1643c8a746d16a204f97bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_scores_dicts = []\n",
    "for string_arr, score_arr in tqdm(zip(scores_data['prediction_strings'], scores_data['scores'])):\n",
    "    ps_pairs = [(p,s) for p,s in zip(string_arr, score_arr)]\n",
    "    ps_pairs = list(set(ps_pairs)) # while sampling, duplicates are created\n",
    "    # remove predictions that are not entities\n",
    "    ps_dict_only_entities = defaultdict(list)\n",
    "    for ps in ps_pairs:\n",
    "        if ps[0] in entity_strings_set:\n",
    "            ps_dict_only_entities[ps[0]] = ps[1]\n",
    "    predictions_scores_dicts.append(ps_dict_only_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45377700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'canada': -3.5002098,\n",
       "             'united states of america': -0.85650057,\n",
       "             'zwe': -8.934294,\n",
       "             'iso 3166-1:au': -2.9644828,\n",
       "             'iriphabliki yesuwela afrika': -5.5159464,\n",
       "             'history of britain (1707-1800)': -5.558312,\n",
       "             'united kingdom of great britain and ireland': -4.3757005,\n",
       "             'etymology of england': -6.1345987,\n",
       "             'current events/brazil/breaking': -8.367983,\n",
       "             \"india's\": -7.577018,\n",
       "             'trinidad and tobago': -7.8673167,\n",
       "             'france': -6.2964683,\n",
       "             'new zealand': -3.6424413})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_scores_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cf0d7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be83a5bfdd914b7ba04ce3be7459970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(74652.5014095095, 1.3452358579214434)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions_filtered = []\n",
    "head_num_filter = 0\n",
    "tail_num_filter = 0\n",
    "for i in tqdm(range(len(predictions_scores_dicts))):\n",
    "    ps_dict = predictions_scores_dicts[i].copy()\n",
    "    target = scores_data['target_strings'][i]\n",
    "    inputs = scores_data['input_strings'][i]\n",
    "    prediction_strings = ps_dict.keys()\n",
    "    if target in prediction_strings:\n",
    "        original_score = ps_dict[target]\n",
    "    # get filtering entities\n",
    "    filtering_entities = getAllFilteringEntities(inputs, filter_dicts)\n",
    "    if 'head' in inputs:\n",
    "        head_num_filter += len(filtering_entities)\n",
    "    else:\n",
    "        tail_num_filter += len(filtering_entities)\n",
    "    for ent in filtering_entities:\n",
    "        if ent in ps_dict:\n",
    "            ps_dict[ent] = -float(\"inf\")\n",
    "    if target in prediction_strings:\n",
    "        ps_dict[target] = original_score\n",
    "    # softmax for scores\n",
    "    names_arr = []\n",
    "    scores_arr = []\n",
    "    for k, v in ps_dict.items():\n",
    "        names_arr.append(k)\n",
    "        scores_arr.append(v)\n",
    "    scores_arr = np.array(scores_arr)\n",
    "#     scores_arr = softmax(scores_arr)\n",
    "    for name, score in zip(names_arr, scores_arr):\n",
    "        ps_dict[name] = score\n",
    "    predictions_filtered.append(ps_dict)\n",
    "head_num_filter/len(predictions_filtered), tail_num_filter/len(predictions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cde8c8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b01b3d1ad641bcbea49cfe4d2189ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "73\n",
      "83\n",
      "86\n",
      "118\n",
      "150\n",
      "216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7204/2220748540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mps_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_strings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfiltering_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAllFilteringEntities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#     if len(filtering_entities) > 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7204/1963677852.py\u001b[0m in \u001b[0;36mgetAllFilteringEntities\u001b[0;34m(input, filter_dicts)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwikidata_link_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "count = {}\n",
    "reciprocal_ranks = 0.0\n",
    "k_list = [1,3,10]\n",
    "for k in k_list:\n",
    "    count[k] = 0\n",
    "num_small_arrs = 0\n",
    "total_count = 0\n",
    "for i in tqdm(range(len(predictions_filtered))):\n",
    "    target = scores_data['target_strings'][i]\n",
    "    ps_dict = predictions_filtered[i]\n",
    "    ps_sorted = sorted(ps_dict.items(), key=lambda item: -item[1])\n",
    "    inputs = scores_data['input_strings'][i]\n",
    "    filtering_entities = getAllFilteringEntities(inputs, filter_dicts)\n",
    "#     if len(filtering_entities) > 1:\n",
    "#         continue\n",
    "#     else:\n",
    "#         total_count += 1\n",
    "    if len(ps_dict) == 0:\n",
    "        preds = []\n",
    "    else:\n",
    "        preds = [x[0] for x in ps_sorted]\n",
    "    if len(filtering_entities) > 1 and target in preds[:1]:\n",
    "        print(i)\n",
    "    if target in preds:\n",
    "        rank = preds.index(target) + 1\n",
    "        reciprocal_ranks += 1./rank\n",
    "    for k in k_list:\n",
    "        if target in preds[:k]:\n",
    "            count[k] += 1\n",
    "    if len(preds) < 10 and target not in preds:\n",
    "        num_small_arrs += 1\n",
    "        \n",
    "total_count = len(predictions_filtered)\n",
    "for k in k_list:\n",
    "    hits_at_k = count[k]/total_count\n",
    "    print('hits@{}'.format(k), hits_at_k)\n",
    "print('mrr', reciprocal_ranks/total_count)\n",
    "print(num_small_arrs/total_count, 'were <10 length preds array without answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fb31c660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4477"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c32486e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict head: circuit de monaco | based in |\n",
      "1990 monaco grand prix\n",
      "1990 monaco grand prix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2007 monaco grand prix', -3.201748),\n",
       " ('1987 monaco grand prix', -3.2927155),\n",
       " ('2004 monaco grand prix', -3.3088353),\n",
       " ('1985 monaco grand prix', -3.3752642),\n",
       " ('2005 monaco grand prix', -3.4837606),\n",
       " ('1995 monaco grand prix', -3.486882),\n",
       " ('2003 monaco grand prix', -3.5745037),\n",
       " ('1997 monaco grand prix', -3.5815535),\n",
       " ('2001 monaco grand prix', -3.6025186),\n",
       " ('1981 monaco grand prix', -3.6073873),\n",
       " ('2006 monaco grand prix', -3.6348004),\n",
       " ('1977 monaco grand prix', -3.6439471),\n",
       " ('2011 monaco grand prix', -3.6837943),\n",
       " ('2015 monaco grand prix', -3.695508),\n",
       " ('1989 monaco grand prix', -3.715921),\n",
       " ('1993 monaco grand prix', -3.7746596),\n",
       " ('2010 monaco grand prix', -3.8480878),\n",
       " ('1991 monaco grand prix', -3.8882747),\n",
       " ('1986 monaco grand prix', -3.9437215),\n",
       " ('2008 monaco grand prix', -3.965727),\n",
       " ('2009 monaco grand prix', -3.9935355),\n",
       " ('1994 monaco grand prix', -4.0105805),\n",
       " ('1988 monaco grand prix', -4.028064),\n",
       " ('1999 monaco grand prix', -4.0320425),\n",
       " ('1975 monaco grand prix', -4.0614038),\n",
       " ('1983 monaco grand prix', -4.0791836),\n",
       " ('1973 monaco grand prix', -4.102641),\n",
       " ('1971 monaco grand prix', -4.1121597),\n",
       " ('1972 monaco grand prix', -4.1360183),\n",
       " ('1996 monaco grand prix', -4.155286),\n",
       " ('1967 monaco grand prix', -4.1563296),\n",
       " ('2013 monaco grand prix', -4.1893735),\n",
       " ('1976 monaco grand prix', -4.209759),\n",
       " ('1974 monaco grand prix', -4.2176323),\n",
       " ('1998 monaco grand prix', -4.226132),\n",
       " ('1984 monaco grand prix', -4.2277474),\n",
       " ('2012 monaco grand prix', -4.2375374),\n",
       " ('2002 monaco grand prix', -4.256293),\n",
       " ('1990 monaco grand prix', -4.2840405),\n",
       " ('1979 monaco grand prix', -4.334267),\n",
       " ('1969 monaco grand prix', -4.3524528),\n",
       " ('1958 monaco grand prix', -4.3554893),\n",
       " ('1965 monaco grand prix', -4.363802),\n",
       " ('1961 monaco grand prix', -4.3671923),\n",
       " ('1992 monaco grand prix', -4.3751564),\n",
       " ('1978 monaco grand prix', -4.3810377),\n",
       " ('2016 monaco grand prix', -4.402445),\n",
       " ('1982 monaco grand prix', -4.4204993),\n",
       " ('1980 monaco grand prix', -4.4276958),\n",
       " ('2000 monaco grand prix', -4.44467),\n",
       " ('2012 italian grand prix', -10.044313),\n",
       " ('1977 italian grand prix', -10.195239),\n",
       " ('1976 italian grand prix', -10.212686)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 83\n",
    "inputs = scores_data['input_strings'][id]\n",
    "predictions_unfiltered = predictions_scores_dicts[id]\n",
    "predictions_unfiltered = [(k,v) for k,v in predictions_unfiltered.items()]\n",
    "preds = predictions_filtered[id]\n",
    "preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "target = scores_data['target_strings'][id]\n",
    "print(inputs)\n",
    "print(target)\n",
    "print(preds[0][0])\n",
    "predictions_unfiltered.sort(key=lambda x:x[1], reverse=True)\n",
    "predictions_unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46ea10f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['research vessel', 'icebreaking', 'm/s/doc']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtering_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8b079bfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict head: dartford | located in the administrative territorial entity |\n",
      "longfield and new barn\n",
      "io e caterina\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('io e caterina', -4.5924783),\n",
       "  ('the bigamist', -4.917049),\n",
       "  ('la figlia del capitano', -5.0445466),\n",
       "  ('noi siamo due evasi', -5.2959394),\n",
       "  ('the great caruso', -5.5247126),\n",
       "  ('la famiglia passaguai fa fortuna', -5.563276),\n",
       "  (\"l'amore difficile\", -5.6025515),\n",
       "  ('django shoots first', -5.604663),\n",
       "  ('io e mia sorella', -5.6096926),\n",
       "  ('la ragazza di bube', -5.6157646)],\n",
       " 'longfield and new barn',\n",
       " ['https://www.wikidata.org/wiki/Q3801125',\n",
       "  'https://www.wikidata.org/wiki/Q3793112',\n",
       "  'https://www.wikidata.org/wiki/Q3822354',\n",
       "  'https://www.wikidata.org/wiki/Q3877777',\n",
       "  'https://www.wikidata.org/wiki/Q1198780',\n",
       "  'https://www.wikidata.org/wiki/Q3822247',\n",
       "  'https://www.wikidata.org/wiki/Q3818471',\n",
       "  'https://www.wikidata.org/wiki/Q1232069',\n",
       "  'https://www.wikidata.org/wiki/Q3801153',\n",
       "  'https://www.wikidata.org/wiki/Q1053810'])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 79\n",
    "inputs = scores_data['input_strings'][id]\n",
    "preds = predictions_filtered[id]\n",
    "preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "target = scores_data['target_strings'][id]\n",
    "print(inputs)\n",
    "print(target)\n",
    "print(preds[0][0])\n",
    "preds[:10], target, [wikidata_link_from_id(e2wdid[x[0]]) for x in preds[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "75768b24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.wikidata.org/wiki/Q134556'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 'single'\n",
    "wikidata_link_from_id(e2wdid[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "b8d6fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 predict tail: ali kazemaini | birthplace | tehran\n",
      "34 predict tail: ashta, maharashtra | instance of | human settlement\n",
      "37 predict tail: roy shaw 0 | instance of | human being\n",
      "40 predict tail: t. canby jones | has surname | jones (family name)\n",
      "45 predict tail: naveen kumar | instance of | human being\n",
      "46 predict tail: barlow respiratory hospital | host country | united states of america\n",
      "48 predict tail: camiling | office held by head of government | mayor\n",
      "51 predict tail: hazel soan | instance of | human being\n",
      "52 predict tail: efrain herrera | sport played | association football\n",
      "53 predict tail: oluf munck | instance of | human being\n",
      "54 predict tail: thomas gilchrist | instance of | human being\n",
      "58 predict tail: desmoplastic fibroma | subclass of | fibroma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('count', 12)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only head/tails\n",
    "count = 0\n",
    "for id in range(60,120, 2):\n",
    "    inputs = scores_data['input_strings'][id]\n",
    "    preds = predictions_filtered[id]\n",
    "    preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "    target = scores_data['target_strings'][id]\n",
    "    pred1 = preds[0][0]\n",
    "    if pred1 == target:\n",
    "        print(int(id/2), inputs, pred1)\n",
    "        count += 1\n",
    "'count', count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "811dccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "print(\"<a href='your_url_here'>Showing Text</a>\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "print(\"<a href='your_url_here'>Showing Text</a>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "36ee8176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4121082'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2wdid['pakistan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8dee3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['english', 'english language', 'french']\n",
    "t = Trie(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fdb192fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6aa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49aba755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d5f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def numLines(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "def loadData(filename, max_points):\n",
    "    file_len = numLines(filename)\n",
    "    f = open(filename, 'r')\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in tqdm(range(file_len)):\n",
    "        if i == max_points:\n",
    "            break\n",
    "        line = f.readline()\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        inputs.append(line[0])\n",
    "        outputs.append(line[1])\n",
    "    data = {'inputs': inputs, 'outputs': outputs}\n",
    "    return data\n",
    "        \n",
    "def load_entity_strings(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def get_entity_wd_id_dict(filename):\n",
    "    out = {}\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        out[line[1]] = line[0]\n",
    "    return out\n",
    "    \n",
    "\n",
    "def create_filter_dict(data) -> Dict[str, int]:\n",
    "    filter_dict = defaultdict(list)\n",
    "    for input, output in tqdm(zip(data[\"inputs\"], data[\"outputs\"])):\n",
    "        filter_dict[input].append(output)\n",
    "    return filter_dict\n",
    "\n",
    "def getAllFilteringEntities(input, filter_dicts):\n",
    "    entities = []\n",
    "    splits = ['train', 'test', 'valid']\n",
    "    for s in splits:\n",
    "        entities.extend(filter_dicts[s][input])\n",
    "    return list(set(entities))\n",
    "\n",
    "def wikidata_link_from_id(id):\n",
    "    uri = 'https://www.wikidata.org/wiki/' + id\n",
    "    return uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'olpbench'\n",
    "# entity_strings = load_entity_strings(os.path.join(\"data\", dataset_name, \"entity_strings.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ee97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_strings_set = set(entity_strings)\n",
    "# len(entity_strings_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51647083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0498d3ec67f748d691766580ae7d6f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=61301566.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8b5a5760d744f2b8624947a9689205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ddc46ad07f486dae67b7ee46c14adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "splits = ['train', 'valid', 'test']\n",
    "dataset_name = 'olpbench'\n",
    "for split in splits:\n",
    "    data[split] = loadData(os.path.join('data', dataset_name, split + '.txt'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616e3b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e91e4b7b54592ae4bc53b770b0004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:02, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55d7cd72cf74719822b666335680ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51fa118e05d425ca0091fc8cc2d8d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_dicts = {}\n",
    "splits = ['train', 'valid', 'test']\n",
    "for split in splits:\n",
    "    filter_dicts[split] = create_filter_dict(data[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a5a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'scores.pickle'\n",
    "# fname = 'scores/scores_full_codexm_small.pickle'\n",
    "fname = 'scores/scores_a10.pickle'\n",
    "# fname = 'scores_500_base_trie.pickle'\n",
    "scores_data = pickle.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28465b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict tail: route 182 | is:impl_appos-clause east of | olive hill\n",
      "['st. thomas aquinas', \"st. john's\", 'san antonio', \"st. john's episcopal church\", \"st. john's lutheran church\", 'st. louis', \"st. thomas'church\", \"st. john's church\", \"st. paul's church\", \"st. mary's\"]\n"
     ]
    }
   ],
   "source": [
    "id = 4\n",
    "print(scores_data['input_strings'][id], scores_data['target_strings'][id])\n",
    "print(scores_data['prediction_strings'][id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9d1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "def load_test_data_olpbench(fname):\n",
    "    # assumes linked data format\n",
    "    # s r o s1|||s2... o1|||o2...\n",
    "    f = open(fname)\n",
    "    tuples = []\n",
    "    for line in f:\n",
    "        if line[-1] == '\\n':\n",
    "            line = line[:-1]\n",
    "        line = line.split('\\t')\n",
    "        tuples.append(line)\n",
    "    out = []\n",
    "    for t in tqdm(tuples):\n",
    "        for s_id, o_id in zip([3,4],[4,3]):\n",
    "            rel = t[1]\n",
    "            s = unidecode(t[s_id]).split('|||')\n",
    "            o = unidecode(t[o_id]).split('|||')\n",
    "            out.append((s, rel, o))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e35ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99fea09ee44e7297a7621e00d27444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = load_test_data_olpbench('data/olpbench/test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aabf38e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dna', 'dna polymerase', 'articulation', 'dna polymerase chain reaction', 'dna damage', 'thorax', 'dna vesicles', 'a thorax', 'articular cartilage', 'dna evidence']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['body flex'], 'is a lack of', ['stiffness', 'rigidity'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 30\n",
    "print(scores_data['prediction_strings'][id])\n",
    "test_data[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c5d0df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores_data['prediction_strings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d535a3de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0827"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for id in range(len(scores_data['prediction_strings'])):\n",
    "    targets = test_data[id][2]\n",
    "    predictions = scores_data['prediction_strings'][id]\n",
    "    # if 1 predictions make array\n",
    "#     predictions = [predictions]\n",
    "    if len(set(targets).intersection(set(predictions))) > 0:\n",
    "        count += 1\n",
    "#         print(id)\n",
    "total_count = len(scores_data['prediction_strings'])\n",
    "count/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acede317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bff88b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61244it [00:11, 5378.53it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_scores_dicts = []\n",
    "for string_arr, score_arr in tqdm(zip(scores_data['prediction_strings'], scores_data['scores'])):\n",
    "    ps_pairs = [(p,s) for p,s in zip(string_arr, score_arr)]\n",
    "    ps_pairs = list(set(ps_pairs)) # while sampling, duplicates are created\n",
    "    # remove predictions that are not entities\n",
    "    ps_dict_only_entities = defaultdict(list)\n",
    "    for ps in ps_pairs:\n",
    "        if ps[0] in entity_strings_set:\n",
    "            ps_dict_only_entities[ps[0]] = ps[1]\n",
    "    predictions_scores_dicts.append(ps_dict_only_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45377700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(predictions_scores_dicts[x]) for x in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f565355b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict tail: vietnam | diplomatic relation | mexico\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'serbia': -3.3982773,\n",
       "             'luxembourg': -4.66638,\n",
       "             'indonesia': -3.5668106,\n",
       "             'germany': -3.5101147,\n",
       "             'mexico': -3.095539,\n",
       "             'brazil': -5.24856,\n",
       "             'italy': -3.5092916,\n",
       "             'ukraine': -4.684395,\n",
       "             'cambodia': -4.270482,\n",
       "             'spain': -3.7536564,\n",
       "             'saudi arabia': -3.193513,\n",
       "             'soviet union': -5.5200253,\n",
       "             'mongolia': -3.908885,\n",
       "             'greece': -5.595957,\n",
       "             'european union': -5.8609905,\n",
       "             'denmark': -3.6262321,\n",
       "             'laos': -3.848981,\n",
       "             'bangladesh': -3.5050812,\n",
       "             'philippines': -3.3096695,\n",
       "             'poland': -5.6494665,\n",
       "             'bulgaria': -3.183054,\n",
       "             'australia': -3.7198315,\n",
       "             'cuba': -4.132122,\n",
       "             'singapore': -4.7629385,\n",
       "             'vietnam': -4.123703,\n",
       "             'turkey': -5.0184784,\n",
       "             'georgia': -4.5141897,\n",
       "             'angola': -5.0547733,\n",
       "             'austria': -3.7758741,\n",
       "             'romania': -4.5870094,\n",
       "             'venezuela': -3.3012657,\n",
       "             'israel': -5.064765,\n",
       "             'india': -3.090887,\n",
       "             \"people's republic of china\": -3.6069994,\n",
       "             'holy see': -4.3225822,\n",
       "             'taiwan': -3.2550182,\n",
       "             'north korea': -3.3946123,\n",
       "             'russia': -3.4549417,\n",
       "             'pakistan': -4.348446,\n",
       "             'united states of america': -3.6606464,\n",
       "             'japan': -4.937191,\n",
       "             'state of palestine': -4.03078,\n",
       "             'france': -3.7842083,\n",
       "             'malaysia': -3.888729,\n",
       "             'south korea': -3.6449738,\n",
       "             'hungary': -3.3495169})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 2\n",
    "print(scores_data['input_strings'][id], scores_data['target_strings'][id])\n",
    "# print(scores_data['scores'][2])\n",
    "predictions_scores_dicts[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89828715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61244"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_scores_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf0d7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 61244/61244 [00:29<00:00, 2083.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374.5308111815034 1.9256090392528247\n",
      "0.36741558356736986\n",
      "14383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions_filtered = []\n",
    "head_num_filter = 0\n",
    "tail_num_filter = 0\n",
    "hits_at_all = 0\n",
    "count = 0\n",
    "for i in tqdm(range(len(predictions_scores_dicts))):\n",
    "    ps_dict = predictions_scores_dicts[i].copy()\n",
    "    target = scores_data['target_strings'][i]\n",
    "    inputs = scores_data['input_strings'][i]\n",
    "    prediction_strings = ps_dict.keys()\n",
    "    if target in prediction_strings:\n",
    "        original_score = ps_dict[target]\n",
    "    # get filtering entities\n",
    "    filtering_entities = getAllFilteringEntities(inputs, filter_dicts)\n",
    "    if len(filtering_entities) == 1:\n",
    "        count += 1\n",
    "    if 'head' in inputs:\n",
    "        head_num_filter += len(filtering_entities)\n",
    "    else:\n",
    "        tail_num_filter += len(filtering_entities)\n",
    "    for ent in filtering_entities:\n",
    "        if ent in ps_dict:\n",
    "            ps_dict[ent] = -float(\"inf\")\n",
    "    if target in prediction_strings:\n",
    "        ps_dict[target] = original_score\n",
    "        hits_at_all += 1\n",
    "    # softmax for scores\n",
    "    names_arr = []\n",
    "    scores_arr = []\n",
    "    for k, v in ps_dict.items():\n",
    "        names_arr.append(k)\n",
    "        scores_arr.append(v)\n",
    "    scores_arr = np.array(scores_arr)\n",
    "#     scores_arr = softmax(scores_arr)\n",
    "    for name, score in zip(names_arr, scores_arr):\n",
    "        ps_dict[name] = score\n",
    "    predictions_filtered.append(ps_dict)\n",
    "print(head_num_filter/len(predictions_filtered), tail_num_filter/len(predictions_filtered))\n",
    "print(hits_at_all/len(predictions_filtered))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde8c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@1 0.2106002220625694\n",
      "hits@3 0.28918751224609757\n",
      "hits@10 0.3459440924825289\n",
      "mrr 0.2571287581304741\n",
      "0.08693096466592645 were <10 length preds array without answer\n"
     ]
    }
   ],
   "source": [
    "count = {}\n",
    "reciprocal_ranks = 0.0\n",
    "k_list = [1,3,10]\n",
    "for k in k_list:\n",
    "    count[k] = 0\n",
    "num_small_arrs = 0\n",
    "count2 = 0\n",
    "for i in range(len(predictions_filtered)):\n",
    "    target = scores_data['target_strings'][i]\n",
    "    ps_dict = predictions_filtered[i]\n",
    "    ps_sorted = sorted(ps_dict.items(), key=lambda item: -item[1])\n",
    "    if len(ps_dict) == 0:\n",
    "        preds = []\n",
    "    else:\n",
    "        preds = [x[0] for x in ps_sorted]\n",
    "    if target in preds:\n",
    "        rank = preds.index(target) + 1\n",
    "        reciprocal_ranks += 1./rank\n",
    "    for k in k_list:\n",
    "        if target in preds[:k]:\n",
    "            count[k] += 1\n",
    "    if len(preds) < 10 and target not in preds:\n",
    "        num_small_arrs += 1\n",
    "    if target in preds:\n",
    "        count2 += 1\n",
    "for k in k_list:\n",
    "    hits_at_k = count[k]/len(predictions_filtered)\n",
    "    print('hits@{}'.format(k), hits_at_k)\n",
    "print('mrr', reciprocal_ranks/len(predictions_filtered))\n",
    "print(num_small_arrs/len(predictions_filtered), 'were <10 length preds array without answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e531dfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4732324701774804"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count2/len(predictions_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b079bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 4\n",
    "inputs = scores_data['input_strings'][id]\n",
    "preds = predictions_filtered[id]\n",
    "preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "target = scores_data['target_strings'][id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75768b24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict tail: Valenciennes | isLocatedIn | Target: Nord-Pas-de-Calais\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('Île-de-France', 0.3138781066564107),\n",
       "  ('Lower_Silesian_Voivodeship', 0.13227073610334242),\n",
       "  ('Nord-Pas-de-Calais', 0.09675851055651631),\n",
       "  ('Valenciennes', 0.07590361486184467),\n",
       "  ('Lists_of_World_Heritage_Sites_in_Europe', 0.061758891746029325),\n",
       "  ('Centre_region,_France', 0.052148043026076424),\n",
       "  ('Lesser_Poland_Voivodeship', 0.044005159013402746),\n",
       "  ('Pays_de_la_Loire', 0.04173769409652669),\n",
       "  ('Paris', 0.03586650379301719),\n",
       "  ('Quebec', 0.030035223333408627)],\n",
       " 'Nord-Pas-de-Calais')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs, 'Target:', target)\n",
    "preds[:10], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "b8d6fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 predict tail: ali kazemaini | birthplace | tehran\n",
      "34 predict tail: ashta, maharashtra | instance of | human settlement\n",
      "37 predict tail: roy shaw 0 | instance of | human being\n",
      "40 predict tail: t. canby jones | has surname | jones (family name)\n",
      "45 predict tail: naveen kumar | instance of | human being\n",
      "46 predict tail: barlow respiratory hospital | host country | united states of america\n",
      "48 predict tail: camiling | office held by head of government | mayor\n",
      "51 predict tail: hazel soan | instance of | human being\n",
      "52 predict tail: efrain herrera | sport played | association football\n",
      "53 predict tail: oluf munck | instance of | human being\n",
      "54 predict tail: thomas gilchrist | instance of | human being\n",
      "58 predict tail: desmoplastic fibroma | subclass of | fibroma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('count', 12)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only head/tails\n",
    "count = 0\n",
    "for id in range(60,120, 2):\n",
    "    inputs = scores_data['input_strings'][id]\n",
    "    preds = predictions_filtered[id]\n",
    "    preds = sorted(preds.items(), key=lambda item: -item[1])\n",
    "    target = scores_data['target_strings'][id]\n",
    "    pred1 = preds[0][0]\n",
    "    if pred1 == target:\n",
    "        print(int(id/2), inputs, pred1)\n",
    "        count += 1\n",
    "'count', count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "811dccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "print(\"<a href='your_url_here'>Showing Text</a>\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "print(\"<a href='your_url_here'>Showing Text</a>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "36ee8176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q4121082'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2wdid['pakistan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8dee3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['english', 'english language', 'french']\n",
    "t = Trie(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fdb192fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.get('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6aa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

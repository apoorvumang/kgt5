{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b7c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eff56811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import BatchEncoding\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "class SentencePieceTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, prefix='wd5m_with_pad', max_tokenize_length=75, pad_to_max = False):\n",
    "        super()\n",
    "        path = os.path.join('data/sentencepiece', prefix + '.model')\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=path)\n",
    "        self._pad_token_id = self.sp['<pad>']\n",
    "        self._eos_token_id = self.sp['</s>']\n",
    "        self.max_tokenize_length = max_tokenize_length\n",
    "        self.pad_to_max = pad_to_max\n",
    "        if self.pad_to_max == True:\n",
    "            print('Max length padding enabled (needed for TPU)')\n",
    "\n",
    "    def make_attention_mask(self, encode_output, max_len):\n",
    "        # padding is with zeros\n",
    "        attention_mask = np.zeros((len(encode_output), max_len), dtype=int)\n",
    "        for i, seq in enumerate(encode_output):\n",
    "            length = min(max_len, len(seq))\n",
    "            attention_mask[i][:length] = np.ones((length), dtype=int)\n",
    "        return attention_mask\n",
    "\n",
    "    # returns BatchEncoding\n",
    "    def __call__(self, text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"):\n",
    "        out = self.sp.encode(text)\n",
    "        # out is a list of list, need to pad and add eos token\n",
    "        for x in out:\n",
    "            x.append(self._eos_token_id)\n",
    "\n",
    "        if self.pad_to_max:\n",
    "            max_len = max_length\n",
    "        else:\n",
    "            max_len = min(max([len(x) for x in out]), self.max_tokenize_length)\n",
    "\n",
    "        attention_mask = self.make_attention_mask(out, max_len)\n",
    "        input_ids = np.ones((len(out), max_len), dtype=int) * self._pad_token_id\n",
    "        for i, seq in enumerate(out):\n",
    "            length = min(max_len, len(seq))\n",
    "            input_ids[i][:length] = seq[:length]\n",
    "\n",
    "#         input_ids = torch.LongTensor(input_ids)\n",
    "#         attention_mask = torch.LongTensor(attention_mask)\n",
    "        data = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        return BatchEncoding(data)\n",
    "\n",
    "    def tokenize_str(self, text):\n",
    "        out = self.sp.encode(text)\n",
    "        return out\n",
    "\n",
    "    def batch_decode(self, input_ids, skip_special_tokens=True):\n",
    "        input_ids = input_ids.tolist()\n",
    "        #TODO: why need to do this?\n",
    "        for i, x in enumerate(input_ids):\n",
    "            if x[0] == 0:\n",
    "                input_ids[i] = input_ids[i][1:]\n",
    "        decoded = self.sp.decode(input_ids)\n",
    "        out = []\n",
    "        for s in decoded:\n",
    "            s = s.replace('<pad>', '')\n",
    "            out.append(s)\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.sp)\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self) -> int:\n",
    "        return self._pad_token_id\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self._eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b44ecfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2ab599deb35af74b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./cache_dir/json/default-2ab599deb35af74b/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to ./cache_dir/json/default-2ab599deb35af74b/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# raw_datasets = load_dataset(\n",
    "#                 'wmt16', 'ro-en', cache_dir='./cache_dir'\n",
    "#         )\n",
    "\n",
    "fname = '/scratche/home/apoorv/transformer-kgc/data/wikidata5m_v3/json_format/train.json'\n",
    "raw_datasets = load_dataset('json', data_files=fname, cache_dir='./cache_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39c10c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5274f607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input', 'output']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "02e05a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42687362"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "71b49e43",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18796/3480407651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "raw_datasets.keys(), raw_datasets['train'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "53f6c68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42687362"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dd43ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a33f42f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length padding enabled (needed for TPU)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "# tokenizer = T5TokenizerFast.from_pretrained('t5-small')\n",
    "tokenizer = SentencePieceTokenizer('sp_wd5m_v3', max_tokenize_length=30, pad_to_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "de570d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['output']\n",
    "    model_inputs = tokenizer(inputs, max_length=30, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    # with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, max_length=30, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     print(model_inputs)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cd6effbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[31190,     4, 28944,     6,   914,     2,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = ['|TAIL| Obama||| position held']\n",
    "tokenizer(inputs, max_length=30, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "578644de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1837, in _map_single\n    writer.write_batch(batch)\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 339, in write_batch\n    self.write_table(pa_table, writer_batch_size)\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 358, in write_table\n    self.pa_writer.write_batch(batch)\n  File \"pyarrow/ipc.pxi\", line 365, in pyarrow.lib._CRecordBatchWriter.write_batch\n  File \"pyarrow/error.pxi\", line 97, in pyarrow.lib.check_status\nOSError: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 174, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/fingerprint.py\", line 340, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1843, in _map_single\n    writer.finalize()\n  File \"/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 368, in finalize\n    self.pa_writer.close()\n  File \"pyarrow/ipc.pxi\", line 402, in pyarrow.lib._CRecordBatchWriter.close\n  File \"pyarrow/error.pxi\", line 97, in pyarrow.lib.check_status\nOSError: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18796/2550585237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataset = train_dataset.map(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mnum_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spawning {} processes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds_per_shard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m                 \u001b[0mtransformed_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating {} shards from multiprocessing\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spawning {} processes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds_per_shard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m                 \u001b[0mtransformed_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating {} shards from multiprocessing\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                num_proc=8,\n",
    "                remove_columns=column_names,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b441da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e16af49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610320"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b964a8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [3229,\n",
       "  361,\n",
       "  266,\n",
       "  2306,\n",
       "  31250,\n",
       "  30988,\n",
       "  21405,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3],\n",
       " 'labels': [1619,\n",
       "  8854,\n",
       "  31407,\n",
       "  31192,\n",
       "  720,\n",
       "  2765,\n",
       "  310,\n",
       "  17578,\n",
       "  31250,\n",
       "  449,\n",
       "  839,\n",
       "  572,\n",
       "  280,\n",
       "  1692,\n",
       "  8148,\n",
       "  1727,\n",
       "  511,\n",
       "  31220,\n",
       "  1122,\n",
       "  5288,\n",
       "  2,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93453bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.map of Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 610320\n",
       "})>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a9d1ecfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find file locally at tsv/tsv.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.6.2/datasets/tsv/tsv.py.\nThe file is also not present on the master branch on github.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mprepare_module\u001b[0;34m(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                     \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't find file at {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0m_raise_if_offline_mode_is_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tried to reach {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.6.2/datasets/tsv/tsv.py",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mprepare_module\u001b[0;34m(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                             \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m                             logger.warning(\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't find file at {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0m_raise_if_offline_mode_is_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tried to reach {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/tsv/tsv.py",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18796/195838187.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0mignore_verifications\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_verifications\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msave_infos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;31m# Download/copy dataset processing script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m     module_path, hash, resolved_file_path = prepare_module(\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0mscript_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscript_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratche/home/apoorv/transformer-kgc/kgc_env/lib/python3.8/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mprepare_module\u001b[0;34m(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                             )\n\u001b[1;32m    342\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m    344\u001b[0m                                 \u001b[0;34m\"Couldn't find file locally at {}, or remotely at {}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                 \"The file is also not present on the master branch on github.\".format(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find file locally at tsv/tsv.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.6.2/datasets/tsv/tsv.py.\nThe file is also not present on the master branch on github."
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder = '/scratche/home/apoorv/transformer-kgc/data/wikidata5m_v3/'\n",
    "splits = ['train_small', 'valid', 'test']\n",
    "data_files = [os.path.join(folder, x + '.txt') for x in splits]\n",
    "\n",
    "dataset = load_dataset('tsv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d5ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

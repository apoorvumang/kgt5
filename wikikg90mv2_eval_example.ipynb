{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95783398-f54a-4c95-ae09-a11b114d4419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4f5c5e92024f50bd8ac52fa32c6c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242083771.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"apoorvumang/kgt5-wikikg90mv2\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"apoorvumang/kgt5-wikikg90mv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80f180d2-0b76-4e28-bf38-fb9d5748a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def getScores(ids, scores, pad_token_id):\n",
    "    \"\"\"get sequence scores from model.generate output\"\"\"\n",
    "    scores = torch.stack(scores, dim=1)\n",
    "    log_probs = torch.log_softmax(scores, dim=2)\n",
    "    # remove start token\n",
    "    ids = ids[:,1:]\n",
    "    # gather needed probs\n",
    "    x = ids.unsqueeze(-1).expand(log_probs.shape)\n",
    "    needed_logits = torch.gather(log_probs, 2, x)\n",
    "    final_logits = needed_logits[:, :, 0]\n",
    "    padded_mask = (ids == pad_token_id)\n",
    "    final_logits[padded_mask] = 0\n",
    "    final_scores = final_logits.sum(dim=-1)\n",
    "    return final_scores.cpu().detach().numpy()\n",
    "\n",
    "def topkSample(input, model, tokenizer, \n",
    "                num_samples=5,\n",
    "                num_beams=1,\n",
    "                max_output_length=30):\n",
    "    tokenized = tokenizer(input, return_tensors=\"pt\")\n",
    "    out = model.generate(**tokenized,\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences = num_samples,\n",
    "                        num_beams = num_beams,\n",
    "                        eos_token_id = tokenizer.eos_token_id,\n",
    "                        pad_token_id = tokenizer.pad_token_id,\n",
    "                        output_scores = True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        max_length=max_output_length,)\n",
    "    out_tokens = out.sequences\n",
    "    out_str = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n",
    "    out_scores = getScores(out_tokens, out.scores, tokenizer.pad_token_id)\n",
    "    \n",
    "    pair_list = [(x[0], x[1]) for x in zip(out_str, out_scores)]\n",
    "    sorted_pair_list = sorted(pair_list, key=lambda x:x[1], reverse=True)\n",
    "    return sorted_pair_list\n",
    "\n",
    "def greedyPredict(input, model, tokenizer):\n",
    "    input_ids = tokenizer([input], return_tensors=\"pt\").input_ids\n",
    "    out_tokens = model.generate(input_ids)\n",
    "    out_str = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n",
    "    return out_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceedab63-495b-4bd4-88a1-145c47f1f2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', -1.093592),\n",
       " ('princess', -1.0935932),\n",
       " ('duke', -2.0669463),\n",
       " ('lady', -3.0623672),\n",
       " ('lord', -5.4861264)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example from validation set that the model predicts correctly\n",
    "# you can try your own examples here. what's your noble title?\n",
    "input = \"Sophie Valdemarsdottir| noble title\"\n",
    "out = topkSample(input, model, tokenizer, num_samples=5)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001f33b-d192-4afd-8d6e-b53dd5d76e2a",
   "metadata": {},
   "source": [
    "You can further load the list of entity aliases, then filter only those predictions which are valid entities \n",
    "then create a reverse mapping from alias -> integer id to get final predictions in required format.\n",
    "\n",
    "However, loading these aliases in memory as a dictionary requires a lot of RAM + you need to download the aliases file \n",
    "\n",
    "The submitted validation/test results were obtained by sampling 300 times for each input, then applying above procedure, followed by filtering known entities. The final MRR can vary slightly due to this sampling nature (we found that although beam search gives deterministic output, the results are inferior to sampling large number of times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57d3885-fd1a-411f-974c-a28766b0fd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-02-15 03:13:37--  https://storage.googleapis.com/kgt5-wikikg90mv2/valid.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.179.80, 142.250.179.112, 216.58.204.112, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.179.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1047144 (1023K) [text/plain]\n",
      "Saving to: ‘valid.txt’\n",
      "\n",
      "valid.txt           100%[===================>]   1023K  2.04MB/s    in 0.5s    \n",
      "\n",
      "2022-02-15 03:13:38 (2.04 MB/s) - ‘valid.txt’ saved [1047144/1047144]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download valid.txt. you can also try same url with test.txt. however test does not contain the correct tails\n",
    "!wget https://storage.googleapis.com/kgt5-wikikg90mv2/valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cd9cc68-9b01-42a1-8f00-c3f230907def",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'valid.txt'\n",
    "valid_lines = []\n",
    "f = open(fname)\n",
    "for line in f:\n",
    "    valid_lines.append(line.rstrip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eb60227-0b2a-438d-82d7-f8acfec0244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untitled Spider-Man: Into the Spider-Verse sequel| director\tKemp Powers\n"
     ]
    }
   ],
   "source": [
    "print(valid_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee7294ab-2c0a-4789-acf8-249f86843bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa767eea19c64bd5972df53b5644d08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hits at 1 unfiltered: 0.135\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# try unfiltered hits@k. this is approximation since model can sample same string multiple times\n",
    "# you should run this on gpu if you want to evaluate on all points with 300 samples each\n",
    "k = 1\n",
    "count_at_k = 0\n",
    "max_predictions = k\n",
    "max_points = 1000\n",
    "for line in tqdm(valid_lines[:max_points]):\n",
    "    input, target = line.split('\\t')\n",
    "    model_output = topkSample(input, model, tokenizer, num_samples=max_predictions)\n",
    "    prediction_strings = [x[0] for x in model_output]\n",
    "    if target in prediction_strings:\n",
    "        count_at_k += 1\n",
    "print('Hits at {0} unfiltered: {1}'.format(k, count_at_k/max_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1d0e8-8600-40e4-bc9e-70cbca0e3ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
